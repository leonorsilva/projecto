{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leonorsilva/projecto/blob/master/Welcome_To_Colaboratory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ],
      "metadata": {
        "id": "cX4JKfsdPOim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 input_dim,\n",
        "                 output_dim,\n",
        "                 hid_dim,\n",
        "                 n_layers,\n",
        "                 n_heads,\n",
        "                 pf_dim,\n",
        "                 dropout,\n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
        "        #self.pos_embedding = nn.Embedding(max_length, hid_dim)  #position embedding\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(hid_dim,\n",
        "                                                  n_heads,\n",
        "                                                  pf_dim,\n",
        "                                                  dropout,\n",
        "                                                  device)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        #src = [batch size, src len]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "\n",
        "        #pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "\n",
        "        #pos = [batch size, src len]\n",
        "\n",
        "        src = self.dropout((self.tok_embedding(src) * self.scale)) #+ self.pos_embedding(pos))\n",
        "\n",
        "        #src = [batch size, src len, hid dim]\n",
        "        print(src)\n",
        "\n",
        "        for layer in self.layers:\n",
        "\n",
        "            src = layer(src, src_mask)\n",
        "\n",
        "        #src = [batch size, src len, hid dim]\n",
        "        output = self.fc_out(src)\n",
        "        output = torch.softmax(torch.mean(output, 1),1)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "TXU491dgPOfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 hid_dim,\n",
        "                 n_heads,\n",
        "                 pf_dim,\n",
        "                 dropout,\n",
        "                 device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.ff_layer_norm = nn.LayerNorm(hid_dim)\n",
        "        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
        "        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim,\n",
        "                                                                     pf_dim,\n",
        "                                                                     dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask):\n",
        "\n",
        "        #src = [batch size, src len, hid dim]\n",
        "        #src_mask = [batch size, 1, 1, src len]\n",
        "\n",
        "        #self attention\n",
        "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
        "        print(src)\n",
        "        print(self.dropout(_src))\n",
        "        #dropout, residual connection and layer norm\n",
        "        src = self.self_attn_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        #src = [batch size, src len, hid dim]\n",
        "\n",
        "        #positionwise feedforward\n",
        "        _src = self.positionwise_feedforward(src)\n",
        "\n",
        "        #dropout, residual and layer norm\n",
        "        src = self.ff_layer_norm(src + self.dropout(_src))\n",
        "\n",
        "        #src = [batch size, src len, hid dim]\n",
        "\n",
        "        return src"
      ],
      "metadata": {
        "id": "3yhu0X3LPOcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
        "        super().__init__()\n",
        "\n",
        "        assert hid_dim % n_heads == 0\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = hid_dim // n_heads\n",
        "\n",
        "        self.fc_q = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_k = nn.Linear(hid_dim, hid_dim)\n",
        "        self.fc_v = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.fc_o = nn.Linear(hid_dim, hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "\n",
        "        batch_size = query.shape[0]\n",
        "\n",
        "        #query = [batch size, query len, hid dim]\n",
        "        #key = [batch size, key len, hid dim]\n",
        "        #value = [batch size, value len, hid dim]\n",
        "\n",
        "        Q = self.fc_q(query)\n",
        "        K = self.fc_k(key)\n",
        "        V = self.fc_v(value)\n",
        "\n",
        "        #Q = [batch size, query len, hid dim]\n",
        "        #K = [batch size, key len, hid dim]\n",
        "        #V = [batch size, value len, hid dim]\n",
        "        print(Q.shape)\n",
        "        print(Q)\n",
        "\n",
        "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        #Q = [batch size, n heads, query len, head dim]\n",
        "        #K = [batch size, n heads, key len, head dim]\n",
        "        #V = [batch size, n heads, value len, head dim]\n",
        "        print(Q.shape)\n",
        "        print(Q)\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "\n",
        "        #energy = [batch size, n heads, query len, key len]\n",
        "        print(energy.shape)\n",
        "\n",
        "        print(mask.shape)\n",
        "        print(mask)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, -1e10)\n",
        "\n",
        "        attention = torch.softmax(energy, dim = -1)\n",
        "\n",
        "        #attention = [batch size, n heads, query len, key len]\n",
        "\n",
        "        x = torch.matmul(self.dropout(attention), V)\n",
        "\n",
        "        #x = [batch size, n heads, query len, head dim]\n",
        "\n",
        "        x = x.permute(0, 2, 1, 3).contiguous()\n",
        "\n",
        "        #x = [batch size, query len, n heads, head dim]\n",
        "\n",
        "        x = x.view(batch_size, -1, self.hid_dim)\n",
        "\n",
        "        #x = [batch size, query len, hid dim]\n",
        "\n",
        "        x = self.fc_o(x)\n",
        "\n",
        "        #x = [batch size, query len, hid dim]\n",
        "\n",
        "        return x, attention"
      ],
      "metadata": {
        "id": "7S1fPSSSPOaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedforwardLayer(nn.Module):\n",
        "    def __init__(self, hid_dim, pf_dim, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n",
        "        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        #x = [batch size, seq len, hid dim]\n",
        "\n",
        "        x = self.dropout(torch.relu(self.fc_1(x)))\n",
        "\n",
        "        #x = [batch size, seq len, pf dim]\n",
        "\n",
        "        x = self.fc_2(x)\n",
        "\n",
        "        #x = [batch size, seq len, hid dim]\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "dcntrgtJPOXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "tFMec7eoPOVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = 15000\n",
        "OUTPUT_DIM = 5\n",
        "HID_DIM = 3\n",
        "ENC_LAYERS = 1\n",
        "ENC_HEADS = 1\n",
        "ENC_PF_DIM = 3\n",
        "ENC_DROPOUT = 0.1\n",
        "\n",
        "enc = Encoder(INPUT_DIM,\n",
        "              OUTPUT_DIM,\n",
        "              HID_DIM,\n",
        "              ENC_LAYERS,\n",
        "              ENC_HEADS,\n",
        "              ENC_PF_DIM,\n",
        "              ENC_DROPOUT,\n",
        "              device)"
      ],
      "metadata": {
        "id": "ZOdMlSOMPOSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "token= tokenizer.encode_plus('I am awesome', max_length=5, truncation=True,\n",
        "                                   padding='max_length', add_special_tokens=True,\n",
        "                                   return_tensors='tf')"
      ],
      "metadata": {
        "id": "INP8CTEAPOQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input=torch.Tensor(token['input_ids'].numpy())\n",
        "input=input.type(torch.int32)\n",
        "\n",
        "mask=torch.Tensor(token['attention_mask'].numpy())\n",
        "mask=mask.type(torch.int32)"
      ],
      "metadata": {
        "id": "-GYNonkqPONq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token['input_ids'].numpy()[0]"
      ],
      "metadata": {
        "id": "On80JhijPOLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input2=torch.Tensor([token['input_ids'].numpy()[0],token['input_ids'].numpy()[0]])\n",
        "input2=input2.type(torch.int32)\n",
        "\n",
        "\n",
        "mask2=torch.Tensor([token['attention_mask'].numpy()[0],token['attention_mask'].numpy()[0]])\n",
        "mask2=mask2.type(torch.int32)"
      ],
      "metadata": {
        "id": "S8n0UoFIPOI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask[0].expand(3,-1)"
      ],
      "metadata": {
        "id": "aIUFQTNJPOGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask=torch.Tensor([mask[0].expand(3,-1)])\n",
        "mask=mask.type(torch.int32)"
      ],
      "metadata": {
        "id": "zj35WoyJPOD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input"
      ],
      "metadata": {
        "id": "bZONZ_GJPOBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "id": "Sl_QpOboPzg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc(input,mask)"
      ],
      "metadata": {
        "id": "bmxro6PjPN-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc(input2,mask2)"
      ],
      "metadata": {
        "id": "D5RZhMxTPN8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train=pd.DataFrame(pd.read_csv('train.tsv',sep='\\t'))"
      ],
      "metadata": {
        "id": "j_KwqbaNPN5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['tokens']=''\n",
        "train['mask']=''"
      ],
      "metadata": {
        "id": "8gCVlsd7PN3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "1ACGSflvPN0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "for i, phrase in enumerate(train['Phrase']):\n",
        "    tokens = tokenizer.encode_plus(phrase, max_length=50, truncation=True,\n",
        "                                   padding='max_length', add_special_tokens=True,\n",
        "                                   return_tensors='tf')\n",
        "    # assign tokenized outputs to respective rows in numpy arrays\n",
        "    train['tokens'][i]=tokens['input_ids']\n",
        "    train['mask'][i]=tokens['attention_mask']\n"
      ],
      "metadata": {
        "id": "lqtCFbIHPNyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IRE5FcqDPNvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tthhi6dmPNtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BY1g1RsCPNqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1aAs9kOFPNoA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}